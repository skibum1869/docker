version: '3.8'
services:

  ollama:
    container_name: ollama
    hostname: ollama
    image: ollama/ollama
    restart:  unless-stopped
    expose:
    - 11434/tcp
    ports:
    - 11434:11434/tcp
    healthcheck:
      test: ollama --version || exit 1
    command: serve
    volumes:
      - ollama:/root/.ollama
    pull_policy: always
    tty: true
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['all']
              capabilities: [gpu]

  webui:
    container_name: webui
    hostname: webui
    image: ghcr.io/open-webui/open-webui:main
    restart:  unless-stopped
    expose:
     - 8080/tcp
    ports:
     - 8080:8080/tcp
    environment:
     - OLLAMA_BASE_URL=http://ollama:11434
     # uncomment the following if you are running ollama on the docker host and remove the ollama service below
     # - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
      - open-webui:/app/backend/data
    depends_on:
     - ollama

  watchtower:
    container_name: watchtower
    hostname: watchtower
    image: containrrr/watchtower
    restart: unless-stopped
    logging:
      options:
        max-size: "10m"
        max-file: "3"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
      - WATCHTOWER_ROLLING_RESTART=true

volumes:
  ollama:
  open-webui:
